{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import time as t\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the same results with train and train_manual_update\n",
    "- Write torch.manual_seed(42) at the beginning of your notebook.\n",
    "- Write torch.set_default_dtype(torch.double) at the beginning of your notebook to alleviate precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3\n",
    "datasets: training, validation and test. Take a subset of these datasets\n",
    "by keeping only 2 labels: cat and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None, verbose=False):\n",
    "    '''\n",
    "    Function for loading the CIFAR-10 dataset, resizing images\n",
    "    to 16x16, and filtering to only include the \"cat\" and \"car\" classes.\n",
    "\n",
    "    Params:\n",
    "    - train_val_split (float): Ratio for splitting the training dataset into.\n",
    "    - data_path (str): Directory where the CIFAR-10 dataset will be downloaded/stored.\n",
    "    - preprocessor (torchvision.transforms.Compose or None): A transform pipeline to preprocess the images.\n",
    "    - verbose (bool): If True, prints information about the loaded dataset.\n",
    "\n",
    "    Returns:\n",
    "    - cifar10_train (list of (Tensor, int)): The filtered training dataset.\n",
    "    - cifar10_val (list of (Tensor, int)): The filtered validation dataset.\n",
    "    - cifar10_test (list of (Tensor, int)): The filtered test dataset.\n",
    "    '''\n",
    "    # If no preprocessor chosen, let our standard one preprocess the dataset:\n",
    "    if preprocessor is None:\n",
    "        # Composing our own transformer:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=(16,16))\n",
    "        ])\n",
    "    \n",
    "    # Loading the train/val and test datasets and downloading if necessary:\n",
    "    cifar10_train_val = datasets.CIFAR10(data_path, train=True, download=True, transform=preprocessor)\n",
    "    cifar10_test = datasets.CIFAR10(data_path, train=False, download=True, transform=preprocessor)\n",
    "\n",
    "    # Split the dataset with given seed:\n",
    "    n_train_val = len(cifar10_train_val)\n",
    "    n_train = int(n_train_val*train_val_split)\n",
    "    n_val = n_train_val - n_train\n",
    "    \n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    cifar10_train, cifar10_val = random_split(cifar10_train_val, lengths=[n_train, n_val], generator=rng)\n",
    "\n",
    "    # Transform from CIFAR-10 --> CIFAR-2:\n",
    "    label_map = {1: 0, 3: 1} # 1 (automobile) is now label 0, 3 (cat) is now label 1:\n",
    "    labels_to_include = [1, 3]\n",
    "    \n",
    "    cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in labels_to_include]\n",
    "    cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in labels_to_include]\n",
    "    cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in labels_to_include]\n",
    "\n",
    "    if verbose:\n",
    "        # Check that preprocessing was successful:\n",
    "        x, label = cifar2_train[0]\n",
    "        unique_labels = len({label for img, label in cifar2_train})\n",
    "        \n",
    "        print(f\"\\nLoading complete!\\nPreprocessed images to shape: Num channels: {x.shape[0]}, (H x W): ({x.shape[1]} x {x.shape[2]})\")\n",
    "        print(f\"Length of datasets:\\n- len(cifar2_train): {len(cifar2_train)}\\n- len(cifar2_val): {len(cifar2_val)}\\n- len(cifar2_test): {len(cifar2_test)}\")\n",
    "        print(f\"Dataset contains {unique_labels} unique labels.\\n\\n\")\n",
    "    \n",
    "    return cifar2_train, cifar2_val, cifar2_test\n",
    "    \n",
    "\n",
    "def compute_accuracy(model, loader, GPU_utilize=False):\n",
    "    '''Compute the accuracy of the chosen model on the current dataloader'''\n",
    "    # If GPU Acceleration available, utilize:\n",
    "    if GPU_utilize:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "        else:\n",
    "            print(\"GPU not available, choosing cpu.\")\n",
    "            device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete!\n",
      "Preprocessed images to shape: Num channels: 3, (H x W): (16 x 16)\n",
      "Length of datasets:\n",
      "- len(cifar2_train): 9017\n",
      "- len(cifar2_val): 983\n",
      "- len(cifar2_test): 2000\n",
      "Dataset contains 2 unique labels.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing load_cifar()\n",
    "cifar_2_train, cifar_2_val, cifar_2_test = load_cifar(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "**Defining plotting functions for visualization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(dataset, class_names, title='Class distribution'):\n",
    "    labels = [label for _, label in dataset]\n",
    "    counter = Counter(labels)\n",
    "    \n",
    "    # Ensure ordering based on class_names keys\n",
    "    keys = sorted(class_names.keys())\n",
    "    counts = [counter.get(k, 0) for k in keys]\n",
    "    names = [class_names[k] for k in keys]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(names, counts, color=['blue', 'orange'][:len(names)])\n",
    "    ax.set_xlabel('Number of samples')\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "# Find one image per label\n",
    "def plot_unique_labels(dataset, class_names):\n",
    "    label_images = {}\n",
    "    for img, label in dataset:\n",
    "        if label not in label_images:\n",
    "            label_images[label] = img\n",
    "        if len(label_images) == 2:  \n",
    "            break\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(1, len(label_images), figsize=(6, 3))\n",
    "    for i, (label, img) in enumerate(label_images.items()):\n",
    "        axes[i].imshow(np.transpose(img.numpy(), (1, 2, 0)))  \n",
    "        axes[i].set_title(f\"Label: {label} ({class_names[label]})\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Utlilizing the functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = {0: \"Automobile\", 1: \"Cat\"}\n",
    "plot_class_distribution(cifar_2_train+cifar_2_val, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_unique_labels(cifar_2_train, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully\n",
    "connected layers) such that:\n",
    "    \n",
    "    - The input dimension is 768(= 16 ∗ 16 ∗ 3) and the output dimension is 2 (for the 2 classes).\n",
    "    - The hidden layers have respectively 128 and 32 hidden units.\n",
    "    - All activation functions are ReLU. The last layer has no activation function since the cross-entropy loss already includes a softmax activation\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    '''\n",
    "    Class implementing a multi layered perceptron, with\n",
    "    dimensions: (input: 768) --> (output: 128)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Inherit functionalities from nn.module:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining dimensions (determined by img size):\n",
    "        n_in = 3 * 16 * 16\n",
    "        n_hidden1 = 128\n",
    "        n_hidden2 = 32\n",
    "        n_out = 2\n",
    "\n",
    "        # Defining layers and activation functions with\n",
    "        # predefined dimensions:\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(n_in, n_hidden1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(n_hidden2, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Model performs a forward pass of x'''\n",
    "        output = self.flat(x)\n",
    "        output = self.activation1(self.fc1(output))\n",
    "        output = self.activation2(self.fc2(output))\n",
    "        return self.fc3(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train(n_epochs, optimizer, model, loss_fn, train_loader) function that trains model for n_epochs epochs given an optimizer optimizer, a loss function loss_fn and a dataloader train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, return_loss=False, verbose=True, GPU_utilize=False):\n",
    "    '''\n",
    "    Function for training a neural network using a given optimizer (e.g. SGD).\n",
    "\n",
    "    Params:\n",
    "    - n_epochs (int): Number of epochs to train.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer used for parameter updates.\n",
    "    - model (torch.nn.Module): The neural network model to train.\n",
    "    - loss_fn (callable): The loss function used during training (e.g. CrossEntropyLoss).\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "    - return_loss (bool): If True, returns a list of average losses per epoch.\n",
    "    - verbose (bool): If True, prints training progress every 10 epochs and at the end.\n",
    "    - GPU_utilize (bool): If True, attempts to train on GPU (CUDA or MPS if available).\n",
    "\n",
    "    Returns:\n",
    "    - train_losses (list of float, optional): Average training loss per epoch.\n",
    "    '''\n",
    "    # If GPU Acceleration available, utilize:\n",
    "    if GPU_utilize:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "        else:\n",
    "            print(\"GPU not available, choosing cpu.\")\n",
    "            device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Initiating default update train function!\")\n",
    "        \n",
    "    # Defining start variables s.a. loss list, num_batches,\n",
    "    # resetting optimizer gradients and initiating training mode:\n",
    "    train_losses = []\n",
    "    num_batches = len(train_loader)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    model.train()\n",
    "\n",
    "    # initiate epoch and batch loops with epoch loss set to 0:\n",
    "    start_time = t.time()\n",
    "    for curr_epoch in range(n_epochs):\n",
    "        curr_loss = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device=device, dtype=torch.double) # As instructed in section 3.3\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            loss = loss_fn(model(x), y) # Compute loss after forward pass\n",
    "            loss.backward()             # Compute gradients\n",
    "            optimizer.step()            # Update model with computed gradients\n",
    "            optimizer.zero_grad()       # Reset gradients\n",
    "            curr_loss += loss.item()   # Update epoch loss w. batch loss\n",
    "\n",
    "        train_losses.append(curr_loss / num_batches) # Append the total epoch loss divided by batches\n",
    "\n",
    "        # If verbosity is true, output:\n",
    "        if verbose:\n",
    "            if curr_epoch+1 == 1 or (curr_epoch+1) % 10 == 0:\n",
    "                print(f\"{datetime.now()} | Curr time used: {(t.time()-start_time):.2f}sec | Epoch num: {curr_epoch+1} | Loss: {curr_loss / num_batches}\")\n",
    "            if curr_epoch+1 == n_epochs:\n",
    "                print(\"Training done!\\n\\n\")\n",
    "\n",
    "    if return_loss:\n",
    "        return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a similar function train manual_update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model using equation (2). Do not forget to zero out all gradients after each iteration. \n",
    "\n",
    "Train 2 instances of MyMLP, one using train and the other using train_manual_update (use the same parameter values for both models). Compare their respective training losses. To get exactly the same results with both functions, see section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0., return_loss=False,\n",
    "                        verbose=True, GPU_utilize=False):\n",
    "    '''\n",
    "    Function for manually updating network weights while training, implementing\n",
    "    a simple SGD (with optional momentum and weight decay).\n",
    "\n",
    "    Params:\n",
    "    - n_epochs (int): Number of epochs to train.\n",
    "    - model (torch.nn.Module): The neural network model to train.\n",
    "    - loss_fn (callable): The loss function used during training.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "    - lr (float): Learning rate for parameter updates.\n",
    "    - momentum_coeff (float): Momentum factor (0 <= momentum_coeff < 1).\n",
    "    - weight_decay (float): Weight decay (L2 regularization) factor (0 <= weight_decay < 1).\n",
    "    - return_loss (bool): If True, returns a list of average losses per epoch.\n",
    "    - verbose (bool): If True, prints training progress every 10 epochs and at the end.\n",
    "    - GPU_utilize (bool): If True, attempts to train on GPU (CUDA or MPS if available).\n",
    "\n",
    "    Returns:\n",
    "    - train_losses (list of float, optional): Average training loss per epoch.\n",
    "    '''\n",
    "    # If GPU Acceleration available, utilize:\n",
    "    if GPU_utilize:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "        else:\n",
    "            print(\"GPU not available, choosing cpu.\")\n",
    "            device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Initiating manual update train function!\")\n",
    "\n",
    "    \n",
    "    # Defining start variables s.a. loss list, num_batches,\n",
    "    # resetting model gradients, momentum buffer and initiating training mode:\n",
    "    train_losses = []\n",
    "    momentum_buffer = [torch.zeros_like(param, device=device, dtype=param.dtype) for param in model.parameters()]\n",
    "    num_batches = len(train_loader)\n",
    "    model.train()\n",
    "\n",
    "    # initiate epoch and batch loops with epoch loss set to 0:\n",
    "    start_time = t.time()\n",
    "    for curr_epoch in range(n_epochs):\n",
    "        curr_loss = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device=device, dtype=torch.double) # As instructed in section 3.3\n",
    "            y = y.to(device=device)\n",
    "            model.zero_grad()          # Resetting gradients\n",
    "            \n",
    "            loss = loss_fn(model(x), y) # Compute loss after forward pass\n",
    "            loss.backward()             # Compute gradients\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for i, (buffer, param) in enumerate(zip(momentum_buffer, model.parameters())):\n",
    "                    gradient = param.grad\n",
    "\n",
    "                    # Adding L2-regularization:\n",
    "                    if weight_decay > 0 and weight_decay < 1:\n",
    "                        gradient = gradient + weight_decay*param.data\n",
    "\n",
    "                    # Adding Momentum:\n",
    "                    if momentum_coeff > 0 and momentum_coeff < 1:\n",
    "                        new_buffer = momentum_coeff * buffer + gradient\n",
    "                        momentum_buffer[i] = new_buffer\n",
    "                        gradient = new_buffer\n",
    "                    \n",
    "                    # Using equation 3 to update: \n",
    "                    # $\\theta_t = \\theta_{t-1}-\\alpha\\nabla L(\\theta_{t-1})$\n",
    "                    param.data -= (lr * gradient)\n",
    "                    \n",
    "            curr_loss += loss.item()   # Update epoch loss w. batch loss\n",
    "\n",
    "        train_losses.append(curr_loss / num_batches) # Append the total epoch loss divided by batches\n",
    "\n",
    "        # If verbosity is true, output:\n",
    "        if verbose:\n",
    "            if curr_epoch+1 == 1 or (curr_epoch+1) % 10 == 0:\n",
    "                print(f\"{datetime.now()} | Curr time used: {(t.time()-start_time):.2f}sec | Epoch num: {curr_epoch+1} | Loss: {curr_loss / num_batches}\")\n",
    "            if curr_epoch+1 == n_epochs:\n",
    "                print(\"Training done!\\n\\n\")\n",
    "    \n",
    "    if return_loss:\n",
    "        return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "### Comparing Loss Between `train` and `train_manual_update`\n",
    "\n",
    "From task 3.1.5:\n",
    "> Train 2 instances of `MyMLP`, one using `train` and the other using `train_manual_update` (use\n",
    "the same parameter values for both models). Compare their respective training losses. To get\n",
    "exactly the same results with both functions, see section 3.3.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Defining loss comparison functions, dual training function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(l_1, l_2, eps):\n",
    "    '''Compare loss between loss lists'''\n",
    "    differences = _epsilon_difference(l_1, l_2, eps)\n",
    "    accumulated_diff = _accumulated_diff(l_1, l_2)\n",
    "    smallest_l_1, smallest_l_2 = min(l_1), min(l_2)\n",
    "    print(f\"\"\"\n",
    "    Loss List Comparison:\n",
    "    ----------------------\n",
    "    - Is the differences smaller than epsilon ({eps}): {differences}\n",
    "    - Accumulated difference between loss lists: {accumulated_diff}\n",
    "\n",
    "    - Smallest loss achieved by model 1: {smallest_l_1}\n",
    "    - Smallest loss achieved by model 2: {smallest_l_2}\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "def _epsilon_difference(l_1, l_2, eps):\n",
    "    '''Private function called by compare_loss to see if list diff is smaller than eps'''\n",
    "    differences = [True if abs(x-y) < eps else False for x, y in zip(l_1, l_2)]\n",
    "    return all(differences)\n",
    "    \n",
    "\n",
    "def _accumulated_diff(l_1, l_2):\n",
    "    '''Private function to return accumulated diff between loss lists'''\n",
    "    return sum(abs(x-y) for x, y in zip(l_1, l_2))\n",
    "    \n",
    "\n",
    "def dual_train_and_compare(n_epochs=60, batch_size=64, learning_rate=1e-2, momentum_coeff=0, l2_coeff=0,\n",
    "                           loss_eps=1e-15, dual=True, return_models=False):\n",
    "    '''\n",
    "    Train one or two instances of MyMLP using PyTorch's SGD and/or a manual\n",
    "    update routine, then compare the resulting losses if both models are trained.\n",
    "\n",
    "    Params:\n",
    "    - n_epochs (int): Number of epochs to train each model.\n",
    "    - batch_size (int): Batch size used in DataLoader.\n",
    "    - learning_rate (float): Learning rate for training.\n",
    "    - momentum_coeff (float): Momentum factor (0 <= momentum_coeff < 1).\n",
    "    - l2_coeff (float): Weight decay factor for L2 regularization (0 <= l2_coeff < 1).\n",
    "    - loss_eps (float): Epsilon threshold for comparing final losses.\n",
    "    - dual (bool): If True, trains two models (default and manual) and compares losses.\n",
    "                   If False, only trains a single model using manual updates.\n",
    "    - return_models (bool): If True, returns the trained model(s).\n",
    "\n",
    "    Returns:\n",
    "    - If return_models=True:\n",
    "        - If dual=True, returns (MyMLP_default_train, MyMLP_manual_train).\n",
    "        - Otherwise, returns (MyMLP_manual_train).\n",
    "      If return_models=False, returns None.\n",
    "    '''\n",
    "    # Initiating two classes of type MyMLP with set seed as instructed\n",
    "    # in section 3.3. aswell as n_epochs and lr:\n",
    "    if dual:\n",
    "        torch.manual_seed(seed)\n",
    "        MyMLP_default_train = MyMLP()\n",
    "        \n",
    "        # Initiating optimizer for the default trained MyMLP:\n",
    "        optimizer = optim.SGD(MyMLP_default_train.parameters(), lr=learning_rate, weight_decay=l2_coeff, momentum=momentum_coeff)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    MyMLP_manual_train = MyMLP()\n",
    "    \n",
    "    # Initiating the dataloader w. shuffle=False for reproducibility:\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_2_train, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initiating loss function:\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Start training on both models w. respective function:\n",
    "    if dual:\n",
    "        loss_default_train = train(n_epochs, optimizer, MyMLP_default_train, loss_fn, train_loader, return_loss=True)\n",
    "    loss_manual_train = train_manual_update(n_epochs, MyMLP_manual_train, loss_fn, train_loader, weight_decay=l2_coeff,\n",
    "                                              momentum_coeff=momentum_coeff, lr=learning_rate, return_loss=True)\n",
    "\n",
    "    if dual:\n",
    "        #Comparing loss\n",
    "        compare_loss(loss_default_train, loss_manual_train, loss_eps)\n",
    "\n",
    "    if return_models:\n",
    "        if dual:\n",
    "            return MyMLP_default_train, MyMLP_manual_train\n",
    "        else:\n",
    "            return MyMLP_manual_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Initiating training and comparing loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating default update train function!\n",
      "2025-02-21 10:26:02.865502 | Curr time used: 0.08sec | Epoch num: 1 | Loss: 0.6759217619022266\n",
      "2025-02-21 10:26:03.350994 | Curr time used: 0.57sec | Epoch num: 10 | Loss: 0.43828589536834245\n",
      "2025-02-21 10:26:03.866682 | Curr time used: 1.08sec | Epoch num: 20 | Loss: 0.33865909447622905\n",
      "2025-02-21 10:26:04.385322 | Curr time used: 1.60sec | Epoch num: 30 | Loss: 0.2826931255278965\n",
      "2025-02-21 10:26:04.898135 | Curr time used: 2.11sec | Epoch num: 40 | Loss: 0.24389811097566635\n",
      "2025-02-21 10:26:05.410094 | Curr time used: 2.63sec | Epoch num: 50 | Loss: 0.21733284623988916\n",
      "2025-02-21 10:26:05.934988 | Curr time used: 3.15sec | Epoch num: 60 | Loss: 0.1944395055375672\n",
      "Training done!\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:05.994009 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6759217619022266\n",
      "2025-02-21 10:26:06.515022 | Curr time used: 0.58sec | Epoch num: 10 | Loss: 0.4382858953683424\n",
      "2025-02-21 10:26:07.095181 | Curr time used: 1.16sec | Epoch num: 20 | Loss: 0.33865909447622905\n",
      "2025-02-21 10:26:07.667379 | Curr time used: 1.73sec | Epoch num: 30 | Loss: 0.2826931255278965\n",
      "2025-02-21 10:26:08.254944 | Curr time used: 2.32sec | Epoch num: 40 | Loss: 0.24389811097566635\n",
      "2025-02-21 10:26:08.837996 | Curr time used: 2.90sec | Epoch num: 50 | Loss: 0.21733284623988913\n",
      "2025-02-21 10:26:09.410437 | Curr time used: 3.48sec | Epoch num: 60 | Loss: 0.19443950553756717\n",
      "Training done!\n",
      "\n",
      "\n",
      "\n",
      "    Loss List Comparison:\n",
      "    ----------------------\n",
      "    - Is the differences smaller than epsilon (1e-15): True\n",
      "    - Accumulated difference between loss lists: 1.8318679906315083e-15\n",
      "\n",
      "    - Smallest loss achieved by model 1: 0.1944395055375672\n",
      "    - Smallest loss achieved by model 2: 0.19443950553756717\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dual_train_and_compare(n_epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Thus, we can conclude that the loss is the same.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "### Comparing Loss Between `train` and `train_manual_update` w. L2 regularization\n",
    "\n",
    "From task 3.1.6:\n",
    "> Modify `train_manual_update` by adding a L2 regularization term in your manual parameter\n",
    "update. Add an additional weight decay parameter to train manual update. Compare\n",
    "again train and train manual update results with 0 <weight decay <1.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Initiating training to compare loss after adding L2 regularization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating default update train function!\n",
      "2025-02-21 10:26:09.472396 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.675926644200173\n",
      "2025-02-21 10:26:09.971639 | Curr time used: 0.56sec | Epoch num: 10 | Loss: 0.4385317661877584\n",
      "2025-02-21 10:26:10.556000 | Curr time used: 1.14sec | Epoch num: 20 | Loss: 0.3389406135986788\n",
      "2025-02-21 10:26:11.126542 | Curr time used: 1.71sec | Epoch num: 30 | Loss: 0.28304796842779756\n",
      "2025-02-21 10:26:11.700847 | Curr time used: 2.29sec | Epoch num: 40 | Loss: 0.2441393368232769\n",
      "2025-02-21 10:26:12.266384 | Curr time used: 2.85sec | Epoch num: 50 | Loss: 0.2177670774352154\n",
      "2025-02-21 10:26:12.885072 | Curr time used: 3.47sec | Epoch num: 60 | Loss: 0.19505564383652813\n",
      "Training done!\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:12.949180 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.675926644200173\n",
      "2025-02-21 10:26:13.525397 | Curr time used: 0.64sec | Epoch num: 10 | Loss: 0.4385317661877584\n",
      "2025-02-21 10:26:14.147614 | Curr time used: 1.26sec | Epoch num: 20 | Loss: 0.3389406135986789\n",
      "2025-02-21 10:26:14.779839 | Curr time used: 1.89sec | Epoch num: 30 | Loss: 0.28304796842779745\n",
      "2025-02-21 10:26:15.394641 | Curr time used: 2.51sec | Epoch num: 40 | Loss: 0.24413933682327696\n",
      "2025-02-21 10:26:16.029008 | Curr time used: 3.14sec | Epoch num: 50 | Loss: 0.21776707743521548\n",
      "2025-02-21 10:26:16.654368 | Curr time used: 3.77sec | Epoch num: 60 | Loss: 0.19505564383652804\n",
      "Training done!\n",
      "\n",
      "\n",
      "\n",
      "    Loss List Comparison:\n",
      "    ----------------------\n",
      "    - Is the differences smaller than epsilon (1e-15): True\n",
      "    - Accumulated difference between loss lists: 1.9984014443252818e-15\n",
      "\n",
      "    - Smallest loss achieved by model 1: 0.19505564383652813\n",
      "    - Smallest loss achieved by model 2: 0.19505564383652804\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dual_train_and_compare(n_epochs=60, l2_coeff=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Thus, we can conclude that the loss is the same.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Comparing Loss Between `train` and `train_manual_update` w. Momentum\n",
    "\n",
    "From task 3.1.7:\n",
    "> Modify `train_manual_update` by adding a L2 regularization term in your manual parameter\n",
    "update. Add an additional weight decay parameter to train manual update. Compare\n",
    "again train and train manual update results with 0 <weight decay <1.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Initiating training to compare loss after adding momentum:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating default update train function!\n",
      "2025-02-21 10:26:16.718230 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6581971647235336\n",
      "2025-02-21 10:26:17.240456 | Curr time used: 0.58sec | Epoch num: 10 | Loss: 0.3527253569061587\n",
      "2025-02-21 10:26:17.821114 | Curr time used: 1.16sec | Epoch num: 20 | Loss: 0.2608193051361419\n",
      "2025-02-21 10:26:18.401634 | Curr time used: 1.74sec | Epoch num: 30 | Loss: 0.21239752240656118\n",
      "2025-02-21 10:26:18.959579 | Curr time used: 2.30sec | Epoch num: 40 | Loss: 0.17791574510793864\n",
      "2025-02-21 10:26:19.561483 | Curr time used: 2.90sec | Epoch num: 50 | Loss: 0.1496621766970712\n",
      "2025-02-21 10:26:20.145429 | Curr time used: 3.49sec | Epoch num: 60 | Loss: 0.12476573049305581\n",
      "Training done!\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:20.205238 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6581971647235336\n",
      "2025-02-21 10:26:20.747890 | Curr time used: 0.60sec | Epoch num: 10 | Loss: 0.35272535690615864\n",
      "2025-02-21 10:26:21.348277 | Curr time used: 1.20sec | Epoch num: 20 | Loss: 0.26081930513614193\n",
      "2025-02-21 10:26:21.966800 | Curr time used: 1.82sec | Epoch num: 30 | Loss: 0.21239752240656118\n",
      "2025-02-21 10:26:22.599125 | Curr time used: 2.45sec | Epoch num: 40 | Loss: 0.17791574510793864\n",
      "2025-02-21 10:26:23.201805 | Curr time used: 3.06sec | Epoch num: 50 | Loss: 0.1496621766970712\n",
      "2025-02-21 10:26:23.818848 | Curr time used: 3.67sec | Epoch num: 60 | Loss: 0.12476573049305578\n",
      "Training done!\n",
      "\n",
      "\n",
      "\n",
      "    Loss List Comparison:\n",
      "    ----------------------\n",
      "    - Is the differences smaller than epsilon (1e-15): True\n",
      "    - Accumulated difference between loss lists: 1.8041124150158794e-15\n",
      "\n",
      "    - Smallest loss achieved by model 1: 0.12476573049305581\n",
      "    - Smallest loss achieved by model 2: 0.12476573049305578\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dual_train_and_compare(n_epochs=60, momentum_coeff=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Thus, we can conclude that the loss is the same.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "### Training different instances of `train` and `train_manual_update` w. different hyperparams:\n",
    "\n",
    "From task 3.1.8:\n",
    ">  Train different instances (at least 4) of the `MyMLP` model with different learning rate, momentum\n",
    "and weight decay values. For hyperparameters values, you can find inspiration in the\n",
    "`gradient_descent_output.txt` file. Note that having different results than in this file is\n",
    "totally normal (e.g. if you had a different dataset split policy than the one used to create this\n",
    "file). However, your `train_manual_update` and `train` functions should give exactly the same\n",
    "results (as we can observe in `gradient_descent_output.txt`)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Initializing values and conducting training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Hyperparam combination 1 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:23.895559 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5665733957127194\n",
      "2025-02-21 10:26:24.537163 | Curr time used: 0.71sec | Epoch num: 10 | Loss: 0.3094579599903425\n",
      "2025-02-21 10:26:25.246247 | Curr time used: 1.42sec | Epoch num: 20 | Loss: 0.2782106429484477\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 2 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:25.318455 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.6344814438671895\n",
      "2025-02-21 10:26:25.949901 | Curr time used: 0.70sec | Epoch num: 10 | Loss: 0.3270450499012421\n",
      "2025-02-21 10:26:26.676971 | Curr time used: 1.43sec | Epoch num: 20 | Loss: 0.2599397849703759\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 3 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:26.738775 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6763797422991122\n",
      "2025-02-21 10:26:27.295349 | Curr time used: 0.62sec | Epoch num: 10 | Loss: 0.4482799483397764\n",
      "2025-02-21 10:26:27.922617 | Curr time used: 1.24sec | Epoch num: 20 | Loss: 0.36137626266154926\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 4 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:27.996451 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5640540370739082\n",
      "2025-02-21 10:26:28.648902 | Curr time used: 0.72sec | Epoch num: 10 | Loss: 0.2749924193090869\n",
      "2025-02-21 10:26:29.381038 | Curr time used: 1.46sec | Epoch num: 20 | Loss: 0.21996016039976585\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 5 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:29.453542 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.6314108968439921\n",
      "2025-02-21 10:26:30.078275 | Curr time used: 0.70sec | Epoch num: 10 | Loss: 0.3062350321687657\n",
      "2025-02-21 10:26:30.777299 | Curr time used: 1.40sec | Epoch num: 20 | Loss: 0.2237794074046938\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 6 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:30.839296 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6759559840767536\n",
      "2025-02-21 10:26:31.395661 | Curr time used: 0.62sec | Epoch num: 10 | Loss: 0.439414693885245\n",
      "2025-02-21 10:26:32.021055 | Curr time used: 1.24sec | Epoch num: 20 | Loss: 0.340930242945548\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 7 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:32.093735 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5636128792321063\n",
      "2025-02-21 10:26:32.727063 | Curr time used: 0.70sec | Epoch num: 10 | Loss: 0.2722684845180394\n",
      "2025-02-21 10:26:33.441249 | Curr time used: 1.42sec | Epoch num: 20 | Loss: 0.20661898021790154\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 8 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:33.510782 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.6312413165603695\n",
      "2025-02-21 10:26:34.119613 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.3050677043094777\n",
      "2025-02-21 10:26:34.787528 | Curr time used: 1.34sec | Epoch num: 20 | Loss: 0.21986563253724853\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 9 ----------\n",
      "- Learning rate: 0.01 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:34.851584 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.675926644200173\n",
      "2025-02-21 10:26:35.411401 | Curr time used: 0.62sec | Epoch num: 10 | Loss: 0.4385317661877584\n",
      "2025-02-21 10:26:36.036100 | Curr time used: 1.25sec | Epoch num: 20 | Loss: 0.3389406135986789\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 10 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:36.103310 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5589297722042795\n",
      "2025-02-21 10:26:36.714472 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.36531329597791024\n",
      "2025-02-21 10:26:37.395031 | Curr time used: 1.36sec | Epoch num: 20 | Loss: 0.3511692547380462\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 11 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:37.465365 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5838422222556231\n",
      "2025-02-21 10:26:38.073231 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.30796250573315204\n",
      "2025-02-21 10:26:38.742003 | Curr time used: 1.35sec | Epoch num: 20 | Loss: 0.2701318589293009\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 12 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:38.801207 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6585952717207425\n",
      "2025-02-21 10:26:39.339503 | Curr time used: 0.60sec | Epoch num: 10 | Loss: 0.3928788629657017\n",
      "2025-02-21 10:26:39.914802 | Curr time used: 1.17sec | Epoch num: 20 | Loss: 0.3101296893311\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 13 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:39.987733 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5561224533451622\n",
      "2025-02-21 10:26:40.617950 | Curr time used: 0.70sec | Epoch num: 10 | Loss: 0.2914412940965054\n",
      "2025-02-21 10:26:41.295854 | Curr time used: 1.38sec | Epoch num: 20 | Loss: 0.23982818792259614\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 14 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:41.366441 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5802386504884242\n",
      "2025-02-21 10:26:41.977753 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.2712333271338963\n",
      "2025-02-21 10:26:42.674615 | Curr time used: 1.38sec | Epoch num: 20 | Loss: 0.20819454776843166\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 15 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:42.738666 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.6571027141479028\n",
      "2025-02-21 10:26:43.305538 | Curr time used: 0.63sec | Epoch num: 10 | Loss: 0.37404276372967105\n",
      "2025-02-21 10:26:43.941623 | Curr time used: 1.27sec | Epoch num: 20 | Loss: 0.28058191536911714\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 16 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:44.010697 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5549676138265592\n",
      "2025-02-21 10:26:44.624325 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.28554800150259935\n",
      "2025-02-21 10:26:45.324118 | Curr time used: 1.38sec | Epoch num: 20 | Loss: 0.22656182400327923\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 17 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:45.392717 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5799303107465165\n",
      "2025-02-21 10:26:46.017475 | Curr time used: 0.69sec | Epoch num: 10 | Loss: 0.2691849603732253\n",
      "2025-02-21 10:26:46.682930 | Curr time used: 1.36sec | Epoch num: 20 | Loss: 0.19674037197953476\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 18 ----------\n",
      "- Learning rate: 0.02 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:46.752990 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.6571520479899056\n",
      "2025-02-21 10:26:47.352242 | Curr time used: 0.67sec | Epoch num: 10 | Loss: 0.3723524288334612\n",
      "2025-02-21 10:26:47.992780 | Curr time used: 1.31sec | Epoch num: 20 | Loss: 0.2766881835960601\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 19 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:48.065589 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5724106226252494\n",
      "2025-02-21 10:26:48.663122 | Curr time used: 0.67sec | Epoch num: 10 | Loss: 0.5764073869985001\n",
      "2025-02-21 10:26:49.373200 | Curr time used: 1.38sec | Epoch num: 20 | Loss: 0.5676067238434482\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 20 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:49.438919 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.5419215194812165\n",
      "2025-02-21 10:26:50.052247 | Curr time used: 0.68sec | Epoch num: 10 | Loss: 0.4202572889383627\n",
      "2025-02-21 10:26:50.739041 | Curr time used: 1.36sec | Epoch num: 20 | Loss: 0.45611193491088287\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 21 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.01 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:50.809557 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5947732261461168\n",
      "2025-02-21 10:26:51.429241 | Curr time used: 0.69sec | Epoch num: 10 | Loss: 0.3857783392399814\n",
      "2025-02-21 10:26:52.039679 | Curr time used: 1.30sec | Epoch num: 20 | Loss: 0.3600996857465111\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 22 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:52.105606 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.5507140109421601\n",
      "2025-02-21 10:26:52.708681 | Curr time used: 0.67sec | Epoch num: 10 | Loss: 0.6952321124596561\n",
      "2025-02-21 10:26:53.420636 | Curr time used: 1.38sec | Epoch num: 20 | Loss: 0.6952324186001109\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 23 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:53.494942 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5419348958251508\n",
      "2025-02-21 10:26:54.147862 | Curr time used: 0.73sec | Epoch num: 10 | Loss: 0.31003103630640066\n",
      "2025-02-21 10:26:54.858411 | Curr time used: 1.44sec | Epoch num: 20 | Loss: 0.2479052021000561\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 24 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:54.929149 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5938252119351515\n",
      "2025-02-21 10:26:55.505740 | Curr time used: 0.65sec | Epoch num: 10 | Loss: 0.32029866897091874\n",
      "2025-02-21 10:26:56.117583 | Curr time used: 1.26sec | Epoch num: 20 | Loss: 0.2523917742098955\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 25 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.9\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:56.187239 | Curr time used: 0.07sec | Epoch num: 1 | Loss: 0.5366696087490407\n",
      "2025-02-21 10:26:56.831960 | Curr time used: 0.71sec | Epoch num: 10 | Loss: 0.33778075923073453\n",
      "2025-02-21 10:26:57.483880 | Curr time used: 1.37sec | Epoch num: 20 | Loss: 0.28683858577882976\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 26 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0.7\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:57.549352 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.5404638971484377\n",
      "2025-02-21 10:26:58.127471 | Curr time used: 0.64sec | Epoch num: 10 | Loss: 0.2867799853939852\n",
      "2025-02-21 10:26:58.843083 | Curr time used: 1.36sec | Epoch num: 20 | Loss: 0.20691925622534293\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------- Hyperparam combination 27 ----------\n",
      "- Learning rate: 0.1 \n",
      "- Weight decay: 0.0001 \n",
      "- Momentum coeff.: 0\n",
      "\n",
      "\n",
      "\n",
      "Initiating manual update train function!\n",
      "2025-02-21 10:26:58.906539 | Curr time used: 0.06sec | Epoch num: 1 | Loss: 0.593309799284881\n",
      "2025-02-21 10:26:59.469108 | Curr time used: 0.62sec | Epoch num: 10 | Loss: 0.3144624477124064\n",
      "2025-02-21 10:27:00.077776 | Curr time used: 1.23sec | Epoch num: 20 | Loss: 0.23118341762041697\n",
      "Training done!\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initializing global params\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "loss_eps = 1e-13\n",
    "\n",
    "# Using iterlist to produce all combinations of hyperparams\n",
    "learning_rates = [0.01, 0.02, 0.1]\n",
    "weight_decays = [0.01, 0.001, 0.0001]\n",
    "momentum_list = [0.9, 0.7, 0]\n",
    "hyperparam_combinations = list(it.product(learning_rates, weight_decays, momentum_list))\n",
    "\n",
    "# Initiating list for holding trained models:\n",
    "models = []\n",
    "\n",
    "# For each combination train with pytorch SGD and manual update:\n",
    "for i, combination in enumerate(hyperparam_combinations):\n",
    "    learning_rate, weight_decay, momentum_coeff = combination[0], combination[1], combination[2]\n",
    "    print(f\"\\n---------- Hyperparam combination {i+1} ----------\")\n",
    "    print(f\"- Learning rate: {learning_rate} \\n- Weight decay: {weight_decay} \\n- Momentum coeff.: {momentum_coeff}\\n\\n\\n\")\n",
    "    \n",
    "    model = dual_train_and_compare(n_epochs=n_epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                   momentum_coeff=momentum_coeff, l2_coeff=weight_decay, loss_eps=loss_eps,\n",
    "                                   dual=False, return_models=True)\n",
    "    print(f\"----------------------------------------------\\n\\n\\n\")\n",
    "    \n",
    "    # Append model with name and params:\n",
    "    models.append((model, \"manual update\", combination))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best trained model and analyzing its performance:\n",
    "\n",
    "From task 3.1.9:\n",
    ">  Select the best model among those trained in the previous question based on their accuracy.\n",
    "\n",
    "From task 3.1.10:\n",
    "> Evaluate the best model and analyse its performance.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Selecting the best trained model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Best performing model:\n",
      "    - Type of update: manual update\n",
      "    \n",
      "    - Params:\n",
      "     -- Learning rate: 0.1\n",
      "     -- Weight decay: 0.0001\n",
      "     -- Momentum coeff.: 0\n",
      "\n",
      "    - Accuracy: 0.9064089521871821\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "def select_best_model(list_of_models):\n",
    "    '''Function for selecting best model'''\n",
    "    # Init values:\n",
    "    best_performing_model = None\n",
    "    best_params = None\n",
    "    best_name = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Initializing dataloader:\n",
    "    loader = torch.utils.data.DataLoader(cifar_2_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # For each model in list of models compute\n",
    "    # accuracy, if best, switch out current best\n",
    "    for model, name, params in list_of_models:\n",
    "        accuracy = compute_accuracy(model, loader)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_performing_model = model\n",
    "            best_params = params\n",
    "            best_name = name\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Best performing model:\n",
    "    - Type of update: {best_name}\n",
    "    \n",
    "    - Params:\n",
    "     -- Learning rate: {params[0]}\n",
    "     -- Weight decay: {params[1]}\n",
    "     -- Momentum coeff.: {params[2]}\n",
    "\n",
    "    - Accuracy: {best_accuracy}\n",
    "     \"\"\")\n",
    "    return best_performing_model\n",
    "\n",
    "best_performing_model = select_best_model(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Evaluating its performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Evaluating our best performing model:\n",
      "    \n",
      "    - Accuracy on unseen test data: 0.896\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "def evaluate_best_performing_model(model):\n",
    "    '''Function for selecting best model'''\n",
    "    # Initializing dataloader:\n",
    "    loader = torch.utils.data.DataLoader(cifar_2_test, batch_size=batch_size, shuffle=False)\n",
    "    accuracy = compute_accuracy(model, loader)\n",
    "\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Evaluating our best performing model:\n",
    "    \n",
    "    - Accuracy on unseen test data: {accuracy}\n",
    "     \"\"\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy_on_test = evaluate_best_performing_model(best_performing_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
